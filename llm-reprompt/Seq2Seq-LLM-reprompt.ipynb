{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139b6d98-b350-4ef9-a824-88b6d2e3ff34",
   "metadata": {},
   "source": [
    "# Seq2Seq LLM Reprompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedad57b-1414-48e1-9d32-d4f1315f6f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "\n",
    "# df.columns\n",
    "ds = load_dataset(\"csv\",data_files={\"train\":'gemma10000.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815cd33e-f94f-4076-902e-8ec1dab1b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df[['original_text','gemma_7b_rewritten_text_temp0','rewrite_prompt']]\n",
    "# df['diff_prompt']='Tell me what prompt converted the following text: \\n\"\"\"' + df['original_text'] + '\"\"\"\\n to this text \\n\"\"\"' + df['gemma_7b_rewritten_text_temp0'] + '\"\"\"\\n'\n",
    "#original text prefix\n",
    "\n",
    "orig_prefix = \"Original Text:\"\n",
    "\n",
    "rewrite_prefix = \"\\nRewritten Text:\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_prompt(orig_text, transformed_text):\n",
    "    message = f\"{orig_prefix} {orig_text} {rewrite_prefix} {transformed_text}\"\n",
    "    #message = f\"{rewrite_prefix} {transformed_text}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for item in ds[\"train\"]:\n",
    "            json_object = {\n",
    "                #\"text\": get_prompt(item[\"original_text\"],item['gemma_7b_rewritten_text_temp0']),\n",
    "                \"text\": get_prompt(item[\"original_text\"],item['rewritten_text']),\n",
    "                \"output\": item['rewrite_prompt']\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./training_dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac41d4be-7887-46c8-a18d-480790dbe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 9999 examples [00:00, 185000.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = load_dataset('json', data_files='./training_dataset.jsonl', split=\"train\")\n",
    "\n",
    "train_dataset=train_dataset.train_test_split(test_size=0.1, seed=37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093ab2f1-d71e-43ed-ae56-0ca366a75287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"google/flan-t5-base\"\n",
    "model_checkpoint = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f58981b-536a-4bcd-9126-3f979497710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 00:45:45.162052: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 00:45:45.162096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 00:45:45.163110: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 00:45:45.168110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 00:45:45.802480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0282eb9-6483-40ed-b942-57b0d53310bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d46c8c-64ce-4e5d-b7db-406f8ed56758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Seq2Seq_bart-large/tokenizer_config.json',\n",
       " './Seq2Seq_bart-large/special_tokens_map.json',\n",
       " './Seq2Seq_bart-large/vocab.json',\n",
       " './Seq2Seq_bart-large/merges.txt',\n",
       " './Seq2Seq_bart-large/added_tokens.json',\n",
       " './Seq2Seq_bart-large/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./Seq2Seq_bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7d0532-118f-48b1-9afb-56db8a2d6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config=AutoConfig.from_pretrained(model_checkpoint)\n",
    "config.save_pretrained('./Seq2Seq_bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32756dc-d894-4024-917e-f41dbb8e5962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-large\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3a94c0-2d79-4ca4-8826-3baba67ab8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7defb2-e45e-4298-8997-0ba0d1a6a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"output\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62e96b0-6a22-48f0-b364-c10a170f77ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8999/8999 [00:01<00:00, 5908.87 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 7227.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3d54b6-145f-4be1-bfcd-ac1f8728b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f637e9d-87c7-4aaa-970f-5a6e25998456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16048b24-1272-4db4-9dd4-f20e1bf3e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-reprompt\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #fp16_full_eval=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9aa8144-98b0-4eb5-94f5-ec263c2f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c177083-8e15-4afc-832b-be1586abc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1160d4f5-a089-4755-9f96-9a0dad3bd1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],    \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a1ffb6-a234-4a20-b334-bb78f3625d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 4:00:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.959300</td>\n",
       "      <td>1.308410</td>\n",
       "      <td>47.886400</td>\n",
       "      <td>31.268700</td>\n",
       "      <td>47.371000</td>\n",
       "      <td>47.414800</td>\n",
       "      <td>12.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.309000</td>\n",
       "      <td>1.159242</td>\n",
       "      <td>47.703300</td>\n",
       "      <td>30.273600</td>\n",
       "      <td>47.186300</td>\n",
       "      <td>47.220400</td>\n",
       "      <td>14.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.145300</td>\n",
       "      <td>1.064047</td>\n",
       "      <td>49.719000</td>\n",
       "      <td>32.970900</td>\n",
       "      <td>49.205000</td>\n",
       "      <td>49.233500</td>\n",
       "      <td>13.936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.059900</td>\n",
       "      <td>0.959474</td>\n",
       "      <td>51.807700</td>\n",
       "      <td>35.497400</td>\n",
       "      <td>51.333800</td>\n",
       "      <td>51.396400</td>\n",
       "      <td>13.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>0.897917</td>\n",
       "      <td>51.506500</td>\n",
       "      <td>35.037400</td>\n",
       "      <td>51.035400</td>\n",
       "      <td>51.076500</td>\n",
       "      <td>13.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>0.851075</td>\n",
       "      <td>54.178900</td>\n",
       "      <td>39.235100</td>\n",
       "      <td>53.804400</td>\n",
       "      <td>53.853600</td>\n",
       "      <td>13.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.797945</td>\n",
       "      <td>54.691500</td>\n",
       "      <td>39.584700</td>\n",
       "      <td>54.341400</td>\n",
       "      <td>54.397800</td>\n",
       "      <td>13.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.764997</td>\n",
       "      <td>55.722000</td>\n",
       "      <td>40.591700</td>\n",
       "      <td>55.391400</td>\n",
       "      <td>55.453000</td>\n",
       "      <td>13.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.741700</td>\n",
       "      <td>0.755768</td>\n",
       "      <td>55.661700</td>\n",
       "      <td>41.199800</td>\n",
       "      <td>55.426600</td>\n",
       "      <td>55.401400</td>\n",
       "      <td>13.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.711700</td>\n",
       "      <td>0.724504</td>\n",
       "      <td>56.548400</td>\n",
       "      <td>41.503300</td>\n",
       "      <td>56.292000</td>\n",
       "      <td>56.316600</td>\n",
       "      <td>13.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.713529</td>\n",
       "      <td>56.678600</td>\n",
       "      <td>42.219000</td>\n",
       "      <td>56.430100</td>\n",
       "      <td>56.433500</td>\n",
       "      <td>13.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.671011</td>\n",
       "      <td>57.821600</td>\n",
       "      <td>43.674600</td>\n",
       "      <td>57.536200</td>\n",
       "      <td>57.547500</td>\n",
       "      <td>13.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>0.646986</td>\n",
       "      <td>57.850100</td>\n",
       "      <td>43.776000</td>\n",
       "      <td>57.634600</td>\n",
       "      <td>57.680400</td>\n",
       "      <td>13.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.593200</td>\n",
       "      <td>0.626873</td>\n",
       "      <td>57.157100</td>\n",
       "      <td>42.633100</td>\n",
       "      <td>56.934700</td>\n",
       "      <td>56.967000</td>\n",
       "      <td>13.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.629149</td>\n",
       "      <td>57.884400</td>\n",
       "      <td>44.082800</td>\n",
       "      <td>57.605200</td>\n",
       "      <td>57.657900</td>\n",
       "      <td>13.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.598973</td>\n",
       "      <td>60.297900</td>\n",
       "      <td>47.200900</td>\n",
       "      <td>60.009600</td>\n",
       "      <td>60.059100</td>\n",
       "      <td>13.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.543300</td>\n",
       "      <td>0.592909</td>\n",
       "      <td>59.313000</td>\n",
       "      <td>46.623500</td>\n",
       "      <td>59.099400</td>\n",
       "      <td>59.215200</td>\n",
       "      <td>13.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>59.288300</td>\n",
       "      <td>45.775000</td>\n",
       "      <td>59.077200</td>\n",
       "      <td>59.099000</td>\n",
       "      <td>14.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.568458</td>\n",
       "      <td>61.553400</td>\n",
       "      <td>48.820500</td>\n",
       "      <td>61.338700</td>\n",
       "      <td>61.319400</td>\n",
       "      <td>13.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.543186</td>\n",
       "      <td>61.844000</td>\n",
       "      <td>49.337200</td>\n",
       "      <td>61.669900</td>\n",
       "      <td>61.707400</td>\n",
       "      <td>13.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.539608</td>\n",
       "      <td>61.913700</td>\n",
       "      <td>49.500200</td>\n",
       "      <td>61.726600</td>\n",
       "      <td>61.788200</td>\n",
       "      <td>13.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.570971</td>\n",
       "      <td>61.618900</td>\n",
       "      <td>48.786400</td>\n",
       "      <td>61.439400</td>\n",
       "      <td>61.484100</td>\n",
       "      <td>13.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.523657</td>\n",
       "      <td>63.117700</td>\n",
       "      <td>50.878100</td>\n",
       "      <td>62.874300</td>\n",
       "      <td>62.928100</td>\n",
       "      <td>13.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.441600</td>\n",
       "      <td>0.511564</td>\n",
       "      <td>62.844100</td>\n",
       "      <td>50.566400</td>\n",
       "      <td>62.663200</td>\n",
       "      <td>62.684500</td>\n",
       "      <td>13.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.417400</td>\n",
       "      <td>0.523191</td>\n",
       "      <td>62.987900</td>\n",
       "      <td>51.017000</td>\n",
       "      <td>62.915500</td>\n",
       "      <td>62.930400</td>\n",
       "      <td>13.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.504885</td>\n",
       "      <td>63.142000</td>\n",
       "      <td>51.426300</td>\n",
       "      <td>62.980500</td>\n",
       "      <td>62.980800</td>\n",
       "      <td>13.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>0.498138</td>\n",
       "      <td>62.894200</td>\n",
       "      <td>51.288600</td>\n",
       "      <td>62.641400</td>\n",
       "      <td>62.656300</td>\n",
       "      <td>13.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.503912</td>\n",
       "      <td>63.909200</td>\n",
       "      <td>52.254000</td>\n",
       "      <td>63.638100</td>\n",
       "      <td>63.709800</td>\n",
       "      <td>13.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.486140</td>\n",
       "      <td>64.783900</td>\n",
       "      <td>53.164400</td>\n",
       "      <td>64.589800</td>\n",
       "      <td>64.620300</td>\n",
       "      <td>13.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.483611</td>\n",
       "      <td>64.869800</td>\n",
       "      <td>53.597400</td>\n",
       "      <td>64.730500</td>\n",
       "      <td>64.766700</td>\n",
       "      <td>13.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>64.274800</td>\n",
       "      <td>52.898300</td>\n",
       "      <td>64.024300</td>\n",
       "      <td>64.079200</td>\n",
       "      <td>13.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.476547</td>\n",
       "      <td>65.313700</td>\n",
       "      <td>54.427100</td>\n",
       "      <td>65.126100</td>\n",
       "      <td>65.113700</td>\n",
       "      <td>13.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.475083</td>\n",
       "      <td>64.703200</td>\n",
       "      <td>53.534900</td>\n",
       "      <td>64.530600</td>\n",
       "      <td>64.490200</td>\n",
       "      <td>13.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.467830</td>\n",
       "      <td>66.012600</td>\n",
       "      <td>54.888700</td>\n",
       "      <td>65.821600</td>\n",
       "      <td>65.906600</td>\n",
       "      <td>13.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.464315</td>\n",
       "      <td>65.654300</td>\n",
       "      <td>54.709800</td>\n",
       "      <td>65.476300</td>\n",
       "      <td>65.494300</td>\n",
       "      <td>13.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.452946</td>\n",
       "      <td>65.785700</td>\n",
       "      <td>55.147900</td>\n",
       "      <td>65.672800</td>\n",
       "      <td>65.659000</td>\n",
       "      <td>13.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.469333</td>\n",
       "      <td>65.598100</td>\n",
       "      <td>54.986200</td>\n",
       "      <td>65.376900</td>\n",
       "      <td>65.450100</td>\n",
       "      <td>14.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.449360</td>\n",
       "      <td>66.120400</td>\n",
       "      <td>55.619000</td>\n",
       "      <td>65.946800</td>\n",
       "      <td>65.965000</td>\n",
       "      <td>13.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.449018</td>\n",
       "      <td>66.057500</td>\n",
       "      <td>55.615900</td>\n",
       "      <td>65.957300</td>\n",
       "      <td>65.976800</td>\n",
       "      <td>13.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.450202</td>\n",
       "      <td>66.973000</td>\n",
       "      <td>56.292700</td>\n",
       "      <td>66.795000</td>\n",
       "      <td>66.857300</td>\n",
       "      <td>13.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.304800</td>\n",
       "      <td>0.444096</td>\n",
       "      <td>67.145100</td>\n",
       "      <td>56.805200</td>\n",
       "      <td>67.058000</td>\n",
       "      <td>67.086200</td>\n",
       "      <td>13.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.444620</td>\n",
       "      <td>67.249600</td>\n",
       "      <td>56.739200</td>\n",
       "      <td>67.137900</td>\n",
       "      <td>67.083200</td>\n",
       "      <td>13.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.437699</td>\n",
       "      <td>67.706600</td>\n",
       "      <td>57.710400</td>\n",
       "      <td>67.594300</td>\n",
       "      <td>67.620100</td>\n",
       "      <td>13.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.290100</td>\n",
       "      <td>0.435228</td>\n",
       "      <td>66.662400</td>\n",
       "      <td>55.971800</td>\n",
       "      <td>66.516800</td>\n",
       "      <td>66.525000</td>\n",
       "      <td>13.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.299600</td>\n",
       "      <td>0.431749</td>\n",
       "      <td>67.636800</td>\n",
       "      <td>57.425700</td>\n",
       "      <td>67.557900</td>\n",
       "      <td>67.562100</td>\n",
       "      <td>13.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.445975</td>\n",
       "      <td>67.478500</td>\n",
       "      <td>57.241300</td>\n",
       "      <td>67.278600</td>\n",
       "      <td>67.357100</td>\n",
       "      <td>14.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.432291</td>\n",
       "      <td>67.275900</td>\n",
       "      <td>57.215900</td>\n",
       "      <td>67.067800</td>\n",
       "      <td>67.098400</td>\n",
       "      <td>13.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.283800</td>\n",
       "      <td>0.432437</td>\n",
       "      <td>67.750300</td>\n",
       "      <td>57.817900</td>\n",
       "      <td>67.593300</td>\n",
       "      <td>67.599700</td>\n",
       "      <td>13.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.432204</td>\n",
       "      <td>67.841400</td>\n",
       "      <td>57.542400</td>\n",
       "      <td>67.679300</td>\n",
       "      <td>67.727800</td>\n",
       "      <td>13.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.429098</td>\n",
       "      <td>68.173300</td>\n",
       "      <td>58.305600</td>\n",
       "      <td>68.004300</td>\n",
       "      <td>68.042400</td>\n",
       "      <td>13.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.435857</td>\n",
       "      <td>68.096200</td>\n",
       "      <td>58.335400</td>\n",
       "      <td>67.931700</td>\n",
       "      <td>68.007400</td>\n",
       "      <td>14.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.430819</td>\n",
       "      <td>68.489600</td>\n",
       "      <td>58.823700</td>\n",
       "      <td>68.311600</td>\n",
       "      <td>68.355700</td>\n",
       "      <td>14.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.429229</td>\n",
       "      <td>68.338100</td>\n",
       "      <td>58.431300</td>\n",
       "      <td>68.112800</td>\n",
       "      <td>68.248300</td>\n",
       "      <td>13.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.418327</td>\n",
       "      <td>68.939600</td>\n",
       "      <td>59.611800</td>\n",
       "      <td>68.865500</td>\n",
       "      <td>68.889300</td>\n",
       "      <td>13.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.434942</td>\n",
       "      <td>69.301600</td>\n",
       "      <td>59.582300</td>\n",
       "      <td>69.162900</td>\n",
       "      <td>69.229500</td>\n",
       "      <td>13.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.419260</td>\n",
       "      <td>68.522200</td>\n",
       "      <td>58.960900</td>\n",
       "      <td>68.412800</td>\n",
       "      <td>68.422000</td>\n",
       "      <td>13.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.422270</td>\n",
       "      <td>68.076600</td>\n",
       "      <td>58.501500</td>\n",
       "      <td>67.923000</td>\n",
       "      <td>67.970800</td>\n",
       "      <td>13.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.420748</td>\n",
       "      <td>68.395100</td>\n",
       "      <td>58.679100</td>\n",
       "      <td>68.215600</td>\n",
       "      <td>68.203100</td>\n",
       "      <td>13.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.240200</td>\n",
       "      <td>0.426036</td>\n",
       "      <td>68.636300</td>\n",
       "      <td>59.042000</td>\n",
       "      <td>68.485100</td>\n",
       "      <td>68.550700</td>\n",
       "      <td>14.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.430970</td>\n",
       "      <td>67.626100</td>\n",
       "      <td>57.778100</td>\n",
       "      <td>67.411300</td>\n",
       "      <td>67.470700</td>\n",
       "      <td>13.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.420380</td>\n",
       "      <td>69.728100</td>\n",
       "      <td>60.336400</td>\n",
       "      <td>69.552000</td>\n",
       "      <td>69.664300</td>\n",
       "      <td>13.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.417464</td>\n",
       "      <td>69.424600</td>\n",
       "      <td>60.068500</td>\n",
       "      <td>69.255600</td>\n",
       "      <td>69.305200</td>\n",
       "      <td>13.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.415520</td>\n",
       "      <td>69.453300</td>\n",
       "      <td>59.839400</td>\n",
       "      <td>69.270000</td>\n",
       "      <td>69.354500</td>\n",
       "      <td>13.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.422938</td>\n",
       "      <td>69.644000</td>\n",
       "      <td>60.218600</td>\n",
       "      <td>69.476300</td>\n",
       "      <td>69.550400</td>\n",
       "      <td>13.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>69.262500</td>\n",
       "      <td>59.975700</td>\n",
       "      <td>69.151100</td>\n",
       "      <td>69.210100</td>\n",
       "      <td>14.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.418511</td>\n",
       "      <td>70.467800</td>\n",
       "      <td>61.322800</td>\n",
       "      <td>70.314800</td>\n",
       "      <td>70.382000</td>\n",
       "      <td>14.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.423809</td>\n",
       "      <td>69.471400</td>\n",
       "      <td>60.289000</td>\n",
       "      <td>69.281000</td>\n",
       "      <td>69.367400</td>\n",
       "      <td>14.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.422754</td>\n",
       "      <td>70.356400</td>\n",
       "      <td>61.010200</td>\n",
       "      <td>70.214000</td>\n",
       "      <td>70.270700</td>\n",
       "      <td>14.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.415020</td>\n",
       "      <td>70.126900</td>\n",
       "      <td>60.732200</td>\n",
       "      <td>69.998400</td>\n",
       "      <td>70.071100</td>\n",
       "      <td>13.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.415093</td>\n",
       "      <td>70.085300</td>\n",
       "      <td>60.881700</td>\n",
       "      <td>69.937200</td>\n",
       "      <td>70.017000</td>\n",
       "      <td>13.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.414848</td>\n",
       "      <td>69.893200</td>\n",
       "      <td>60.562500</td>\n",
       "      <td>69.756400</td>\n",
       "      <td>69.814900</td>\n",
       "      <td>14.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.413390</td>\n",
       "      <td>69.434500</td>\n",
       "      <td>60.015100</td>\n",
       "      <td>69.261100</td>\n",
       "      <td>69.363000</td>\n",
       "      <td>13.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.418438</td>\n",
       "      <td>70.450900</td>\n",
       "      <td>61.208300</td>\n",
       "      <td>70.244200</td>\n",
       "      <td>70.332800</td>\n",
       "      <td>13.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>69.787300</td>\n",
       "      <td>60.452600</td>\n",
       "      <td>69.622900</td>\n",
       "      <td>69.721300</td>\n",
       "      <td>13.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.417588</td>\n",
       "      <td>69.881700</td>\n",
       "      <td>60.638400</td>\n",
       "      <td>69.712800</td>\n",
       "      <td>69.779400</td>\n",
       "      <td>13.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.417959</td>\n",
       "      <td>70.672600</td>\n",
       "      <td>61.585100</td>\n",
       "      <td>70.516100</td>\n",
       "      <td>70.553500</td>\n",
       "      <td>13.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.413966</td>\n",
       "      <td>70.197300</td>\n",
       "      <td>60.928900</td>\n",
       "      <td>69.984100</td>\n",
       "      <td>70.103100</td>\n",
       "      <td>13.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>70.371100</td>\n",
       "      <td>61.337500</td>\n",
       "      <td>70.211000</td>\n",
       "      <td>70.300800</td>\n",
       "      <td>13.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.414895</td>\n",
       "      <td>70.656500</td>\n",
       "      <td>61.566300</td>\n",
       "      <td>70.501700</td>\n",
       "      <td>70.580200</td>\n",
       "      <td>13.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>70.682700</td>\n",
       "      <td>61.670400</td>\n",
       "      <td>70.520500</td>\n",
       "      <td>70.594100</td>\n",
       "      <td>13.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.414351</td>\n",
       "      <td>70.281200</td>\n",
       "      <td>61.160100</td>\n",
       "      <td>70.096100</td>\n",
       "      <td>70.187700</td>\n",
       "      <td>13.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.417087</td>\n",
       "      <td>70.314100</td>\n",
       "      <td>61.023800</td>\n",
       "      <td>70.115200</td>\n",
       "      <td>70.224800</td>\n",
       "      <td>13.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.419626</td>\n",
       "      <td>70.727700</td>\n",
       "      <td>61.646200</td>\n",
       "      <td>70.556800</td>\n",
       "      <td>70.635900</td>\n",
       "      <td>13.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.418173</td>\n",
       "      <td>70.582000</td>\n",
       "      <td>61.441800</td>\n",
       "      <td>70.444900</td>\n",
       "      <td>70.486500</td>\n",
       "      <td>13.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.423204</td>\n",
       "      <td>70.587100</td>\n",
       "      <td>61.360400</td>\n",
       "      <td>70.438400</td>\n",
       "      <td>70.483500</td>\n",
       "      <td>13.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.418447</td>\n",
       "      <td>70.600300</td>\n",
       "      <td>61.389100</td>\n",
       "      <td>70.419000</td>\n",
       "      <td>70.536200</td>\n",
       "      <td>13.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.417936</td>\n",
       "      <td>70.539000</td>\n",
       "      <td>61.481000</td>\n",
       "      <td>70.373000</td>\n",
       "      <td>70.484000</td>\n",
       "      <td>13.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.418962</td>\n",
       "      <td>70.559100</td>\n",
       "      <td>61.448800</td>\n",
       "      <td>70.413900</td>\n",
       "      <td>70.509000</td>\n",
       "      <td>13.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.419261</td>\n",
       "      <td>70.671400</td>\n",
       "      <td>61.538900</td>\n",
       "      <td>70.519100</td>\n",
       "      <td>70.603900</td>\n",
       "      <td>13.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.419074</td>\n",
       "      <td>70.614800</td>\n",
       "      <td>61.481800</td>\n",
       "      <td>70.473200</td>\n",
       "      <td>70.597200</td>\n",
       "      <td>13.964000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory bart-large-finetuned-reprompt/checkpoint-5000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory bart-large-finetuned-reprompt/checkpoint-5500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45000, training_loss=0.39413419172498915, metrics={'train_runtime': 14431.1857, 'train_samples_per_second': 12.472, 'train_steps_per_second': 3.118, 'total_flos': 1.546447092691845e+17, 'train_loss': 0.39413419172498915, 'epoch': 20.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c194e093-71f5-4189-ab14-66af2b4ae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('Seq2Seq_bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b41bb6b2-5505-49ef-aaeb-da37301a0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./Seq2Seq_bart-large', device_map=\"auto\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large', device_map=\"auto\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./Seq2Seq_bart-large', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93dc51fb-08b9-40e3-965b-7794fed80fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    input_ids = tokenizer(x, return_tensors='pt').input_ids.to('cuda')\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_target_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b230bec-b1e3-4a63-b736-d99af7a95580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change the text into a TV shopping network product pitch\n",
      "Turn the content into a reality TV show pitch\n",
      "Original Text: Unless otherwise stated, the content of this page is licensed under Creative Commons Attribution-ShareAlike 3.0 License The SCP Foundation Wiki Sigma-9 theme and style was designed by Aelanna , and used under the Creative Commons Attribution-ShareAlike 3.0 license ( CC BY-SA 3.0 ). Branches of the SCP Foundation The results of the Community Survey 2018 are here ! The Review Team needs YOU as a new recruit! The International Translation Archive is now represented on the English Wiki and regularly uploads translations from INT to EN! The International Translation Archive contains translations of SCP articles, tales, canons and other articles \n",
      "Rewritten Text: ## SCP Foundation: The Global Hunt Begins!\n",
      "\n",
      "**Introducing the International Translation Archive, a collaborative platform that bridges language barriers and expands our understanding of the SCP Foundation universe!**\n",
      "\n",
      "Join the Review Team and help shape the future of SCP fandom by translating SCP articles, tales, canons, and more into diverse languages. We are seeking passionate individuals who are fluent in multiple languages to join our diverse team and spread the word about the SCP Foundation.\n",
      "\n",
      "**Here's what you can do:**\n",
      "\n",
      "* **Translate SCP articles, tales, and other articles from English to your native language.**\n",
      "* **Help expand the reach of the SCP Foundation by making it accessible to a wider audience.**\n",
      "* **Share your love for SCP with the world by contributing to a vibrant and inclusive community.**\n",
      "* **Be part of a global effort to preserve and celebrate the SCP Foundation's legacy.**\n",
      "\n",
      "**Benefits of joining the Review Team:**\n",
      "\n",
      "* **Make a real difference in the SCP community.**\n",
      "* **Contribute to the growth of the SCP Foundation Wiki.**\n",
      "* **Network with fellow SCP enthusiasts from around the globe.**\n",
      "* **Gain valuable experience in translation and cultural exchange.**\n",
      "\n",
      "**Are you ready to take on the challenge? Apply today!**\n",
      "\n",
      "**Applications are open\n"
     ]
    }
   ],
   "source": [
    "print(pred(train_dataset[\"train\"][0][\"text\"]))\n",
    "print(train_dataset[\"train\"][0][\"output\"])\n",
    "print(train_dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff820a3-8538-476a-a637-0b73eed11029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite the text in reverse word order\n",
      "Rewrite the text in reverse word order\n",
      "Original Text: Please enable Javascript to watch this video SEATTLE -- A hero in America's war in Afghanistan is now delivering for his 1-year-old nephew. Little Hudson Hill is alive, thanks to his uncle, Lt. Col. Trevor Hill, who donated a part of his liver as a living donor. Trevor underwent surgery at UW Medical Center. Hudson's surgery was 2 miles away at Seattle Children's Hospital. Trevor is an Army Green Beret and says he was motivated to help his nephew, in part, because others saved his life in the battlefied. But he also cites something else -- survivor's guilt. Surgeons removed \n",
      "Rewritten Text: \n",
      "Sure, here's the reversed text:\n",
      "\n",
      "Lt. Col. Trevor Hill donated a part of his liver to save his 1-year-old nephew, Hudson Hill. In the battle-stricken war in Afghanistan, he was motivated by the fact that others had saved his life, which was what drove him to help Hudson. Surgeons removed two miles of tissue from Trevor's liver to save Hudson's life.\n"
     ]
    }
   ],
   "source": [
    "i=72\n",
    "print(pred(train_dataset[\"train\"][i][\"text\"]))\n",
    "print(train_dataset[\"train\"][i][\"output\"])\n",
    "print(train_dataset[\"train\"][i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2381e5b-18fe-485d-8da7-3956e4c071b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
