{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139b6d98-b350-4ef9-a824-88b6d2e3ff34",
   "metadata": {},
   "source": [
    "# Mistral Guanoca LoRA PEFT example\n",
    "\n",
    "Source Article: https://www.datacamp.com/tutorial/mistral-7b-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89e65d2-d0ef-428a-9582-537ff3784f34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(  \n\u001b[1;32m      8\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     11\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,\n\u001b[1;32m     20\u001b[0m                                           trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m                                           device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m                                          )\n\u001b[1;32m     23\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/envs/torch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/envs/torch/lib/python3.11/site-packages/transformers/modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/envs/torch/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          trust_remote_code=True,\n",
    "                                          device_map=\"auto\"\n",
    "                                         )\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "print(tokenizer.add_bos_token, tokenizer.add_eos_token)\n",
    "\n",
    "model.config.use_cache = False # silence the warnings\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedad57b-1414-48e1-9d32-d4f1315f6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "\n",
    "ds = load_dataset(\"csv\",data_files={\"train\":'gemma10000.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b91a51-72b3-4399-a192-c55bc39c54c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convert the text into a vintage circus poster announcement'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['rewrite_prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "815cd33e-f94f-4076-902e-8ec1dab1b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "orig_prefix = \"Original Text: \"\n",
    "\n",
    "#mistral \"response\"\n",
    "llm_response_for_rewrite = \"Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.\"\n",
    "\n",
    "#modified text prefix\n",
    "rewrite_prefix = \"\\nRewritten Text: \"\n",
    "\n",
    "#provided as start of Mistral response (anything after this is used as the prompt)\n",
    "#providing this as the start of the response helps keep things relevant\n",
    "response_start = \"The request was: \"\n",
    "\n",
    "#We insert our detected prompt into this well-scoring baseline text\n",
    "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
    "base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.' \n",
    "base_line_swap_text = \"[insert desired style here]\"\n",
    "\n",
    "#first example\n",
    "example_text_1 = \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\"\n",
    "example_rewrite_1 = \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\"\n",
    "example_prompt_1 = \"Shift the narrative from offering an informal, comforting assurance to providing a more structured advisory note that emphasizes mindfulness and preparedness. This modification should mark a clear change in tone and purpose, aiming to responsibly inform and guide, while maintaining a positive and constructive approach.\"\n",
    "\n",
    "#second example\n",
    "example_text_2 = \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\"\n",
    "example_rewrite_2 = \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\"\n",
    "example_prompt_2 = \"Transform your communication from an academic delivery to a dynamic, rhythm-infused presentation. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.\"\n",
    "\n",
    "\n",
    "# this function is used to output the right formate for each row in the dataset\n",
    "def create_text_row(instruction, output, complete=True) -> str:\n",
    "    if complete:\n",
    "        text_row = f\"\"\"<s>[INST] {instruction} [/INST] \\n {output} </s>\"\"\"\n",
    "    else:\n",
    "        text_row = f\"\"\"<s>[INST] {instruction} [/INST] \\n {output} \"\"\"\n",
    "    return text_row\n",
    "\n",
    "\n",
    "def get_prompt(orig_text, transformed_text, rewritten_prompt = None):\n",
    "    message = create_text_row(f\"{orig_prefix} {orig_text}\", llm_response_for_rewrite)\n",
    "    if rewritten_prompt is None:\n",
    "        message = message + create_text_row(f\"{rewrite_prefix} {transformed_text}\", response_start, False)\n",
    "    else:\n",
    "        message = message + create_text_row(f\"{rewrite_prefix} {transformed_text}\", f\"{response_start} {rewritten_prompt}\")\n",
    "    return message\n",
    "\n",
    "\n",
    "def chat(messages):\n",
    "    encodeds = tokenizer(messages, return_tensors=\"pt\")\n",
    "    \n",
    "    model_inputs = encodeds.to(torch.cuda.current_device())\n",
    "    \n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=2000, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    print(encodeds[0])\n",
    "    print(decoded[0])\n",
    "    return decoded[0]\n",
    "\n",
    "chat(messages)\n",
    "\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        i=0\n",
    "        for item in ds[\"train\"]:\n",
    "            json_object = {\n",
    "                \"text\": get_prompt(item[\"original_text\"],item['rewritten_text'],item['rewrite_prompt'])\n",
    "            }\n",
    "            if i==0:\n",
    "                chat(json_object)\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "            i+=1\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./training_dataset.jsonl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac41d4be-7887-46c8-a18d-480790dbe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 9999 examples [00:00, 221916.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files='./training_dataset.jsonl' , split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edc4f662-b63b-4e53-8be3-cdd2d12c1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.train_test_split(test_size=0.1, seed=37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f109dc69-0bcc-499c-b3e5-63570fc673d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 01:14:54.386645: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-27 01:14:54.386688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-27 01:14:54.387664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-27 01:14:54.391447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 01:14:55.040723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6da41dc8-f7b0-4703-8613-a8c7018726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af4f26a-1c12-47ae-b6cd-20a9e5fe6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #optim=\"paged_adamw_32bit\",\n",
    "    save_total_limit = 2, \n",
    "    save_strategy = \"steps\", \n",
    "    load_best_model_at_end=False,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c743de9-b60c-4b26-a7f4-062561b64e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8999/8999 [00:00<00:00, 9157.25 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 9150.46 examples/s]\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a27ea0-ddb2-451e-ab5b-23da781a745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 1:29:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.366800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "new_model='Mistral-7b-instruct-v0.1-qlora'\n",
    "#new_model='Mistral-7b-v0.1-qlora'\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf982bc7-4e7a-471e-a8d7-9716b01822cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.model',\n",
       " 'merged_model/added_tokens.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the model with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16, #bfloat and float?\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3a42fbc-b75b-4c7b-b766-ac272afae12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] Original Text:  Albuquerque police have released lapel camera footage that shows an officer shooting at a fleeing suspect. In October 2013, officers were called about an erratic driver who was running drivers off the road and firing a weapon from his vehicle. Officers said the man, Joaquin Ortega, tried to carjack drivers after crashing his own car. The newly-released footage seems to contradict the story that police initially reported. Then-Interim Police Chief Allen Banks would not release more than a single frame of the lapel video after the incident, stating that he wasn’t going to allow the case to be tried in [/INST] \n",
      " Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral. </s><s> [INST] \n",
      "Rewritten Text:  \n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  </s><s> [INST] \n",
      "Rewritten Text:  \n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps,\n",
      "(' ',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] Original Text:  Albuquerque police have released lapel camera footage that shows an officer shooting at a fleeing suspect. In October 2013, officers were called about an erratic driver who was running drivers off the road and firing a weapon from his vehicle. Officers said the man, Joaquin Ortega, tried to carjack drivers after crashing his own car. The newly-released footage seems to contradict the story that police initially reported. Then-Interim Police Chief Allen Banks would not release more than a single frame of the lapel video after the incident, stating that he wasn’t going to allow the case to be tried in [/INST] \n",
      " Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral. </s><s> [INST] \n",
      "Rewritten Text:  \n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  </s><s> [INST] \n",
      "Rewritten Text:  \n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps, except for the last sentence  [/INST] \n",
      " The request was:  \n",
      " Turn the text to all caps,\n",
      "(' ',)\n",
      "<s><s> [INST] Original Text:  Cork City FC are pleased to announce that Colin Healy has signed a new contract with the club. The midfielder made twenty league appearances for the club last season, and speaking to CorkCityFC.ie, Healy said: “I am very pleased to have signed back with Cork City for the 2016 season. We are all looking forward to the new season and hopefully we can have a good year.” City boss John Caulfield praised Healy’s positive influence at the club, saying: “Since I came in, Colin has been fantastic, both on the pitch and off it. In 2014, he had a fantastic [/INST] \n",
      " Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral. </s><s> [INST] \n",
      "Rewritten Text:  ## Cork City FC Community Guidelines:\n",
      "\n",
      "**Respect and Courtesy:**\n",
      "- Treat all members with respect, regardless of their opinions or viewpoints.\n",
      "- Be mindful of your tone and language, be inclusive and welcoming to all.\n",
      "- Avoid personal attacks, bullying, or harassment.\n",
      "\n",
      "**Sportsmanship and Fair Play:**\n",
      "- Adhere to the laws and spirit of fair play in all competitive and recreational activities.\n",
      "- Show respect for opponents, officials, and volunteers.\n",
      "- Cooperate with teammates, coaches, and opponents.\n",
      "\n",
      "**Content and Behavior:**\n",
      "- Use only appropriate and relevant content.\n",
      "- Be mindful of your own words and actions, and avoid spreading misinformation or hate speech.\n",
      "- Report any inappropriate or offensive behavior or content immediately.\n",
      "\n",
      "**Communication:**\n",
      "- Keep your posts relevant to Cork City FC and the local football community.\n",
      "- Be respectful and courteous when communicating with other members.\n",
      "- Avoid self-promotion or spam.\n",
      "\n",
      "**Community Building:**\n",
      "- Engage with the club and its supporters by commenting on news, events, and player updates.\n",
      "- Support the team by attending matches and volunteering your time.\n",
      "- Promote a positive and inclusive atmosphere.\n",
      "\n",
      "**Privacy and Security:**\n",
      "- Respect the privacy of individuals and avoid [/INST] \n",
      " The request was:  </s><s> [/INST] \n",
      " The text was:  ## Cork City FC Community Guidelines:\n",
      "\n",
      "**Respect and Courtesy:**\n",
      "- Treat all members with respect, regardless of their opinions or viewpoints.\n",
      "- Be mindful of your tone and language, be inclusive and welcoming to all.\n",
      "- Avoid personal attacks, bullying, or harassment.\n",
      "\n",
      "**Sportsmanship and Fair Play:**\n",
      "- Adhere to the laws and spirit of fair play in all competitive and recreational activities.\n",
      "- Show respect for opponents, officials, and volunteers.\n",
      "- Cooperate with teammates, coaches, and opponents.\n",
      "\n",
      "**Content and Behavior:**\n",
      "- Use only appropriate and relevant content.\n",
      "- Be mindful of your own words and actions, and avoid spreading misinformation or hate speech.\n",
      "- Report any inappropriate or offensive behavior or content immediately.\n",
      "\n",
      "**Communication:**\n",
      "- Keep your posts relevant to Cork City FC and the local football community.\n",
      "- Be respectful and courteous when communicating with other members.\n",
      "- Avoid self-promotion or spam.\n",
      "\n",
      "**Community Building:**\n",
      "- Engage with the club and its supporters by commenting on news, events, and player updates.\n",
      "- Support the team by attending matches and volunteering your time.\n",
      "- Promote a positive and inclusive atmosphere.\n",
      "\n",
      "**Privacy and Security:**\n",
      "- Respect the privacy of individuals and avoid sharing personal information.\n",
      "- Avoid sharing sensitive or confidential information.\n",
      "- Report any suspicious or inappropriate behavior immediately. [/INST] \n",
      " The request was:  Rewrite the message as a social media platform's community guidelines  [/INST] \n",
      " The text was:  ## Cork City FC Community Guidelines:\n",
      "\n",
      "**Respect and Courtesy:**\n",
      "- Treat all members with respect, regardless of their opinions or viewpoints.\n",
      "- Be mindful of your tone and language, be inclusive and welcoming to all.\n",
      "- Avoid personal attacks, bullying, or harassment.\n",
      "\n",
      "**Sportsmanship and Fair Play:**\n",
      "- Adhere to the laws and spirit of fair play in all competitive and recreational activities.\n",
      "- Show respect for opponents, officials, and volunteers.\n",
      "- Cooperate with teammates, coaches, and opponents.\n",
      "\n",
      "**Content and Behavior:**\n",
      "- Use only appropriate and relevant content.\n",
      "- Be mindful of your own words and actions, and avoid spreading misinformation or hate speech.\n",
      "- Report any inappropriate or offensive behavior or content immediately.\n",
      "\n",
      "**Communication:**\n",
      "- Keep your posts relevant to Cork City FC and the local football community.\n",
      "- Be respectful and courteous when communicating with other members.\n",
      "- Avoid self-promotion or spam.\n",
      "\n",
      "**Community Building:**\n",
      "- Engage with the club and its supporters by commenting on news, events, and player updates.\n",
      "- Support the team by attending matches and volunteering your time.\n",
      "- Promote a positive and inclusive atmosphere.\n",
      "\n",
      "**Privacy and Security:**\n",
      "- Respect the privacy of individuals and avoid sharing personal information.\n",
      "- Avoid sharing sensitive or confidential information.\n",
      "- Report any suspicious or inappropriate behavior immediately. [/INST] \n",
      " The request was:  Rewrite the message as a social media platform's community guidelines  [/INST] \n",
      " The text was:  ## Cork City FC Community Guidelines:\n",
      "\n",
      "**Respect and Courtesy:**\n",
      "- Treat all members with respect, regardless of their opinions or viewpoints.\n",
      "- Be mindful of your tone and language, be inclusive and welcoming to all.\n",
      "- Avoid personal attacks, bullying, or harassment.\n",
      "\n",
      "**Sportsmanship and Fair Play:**\n",
      "- Adhere to the laws and spirit of fair play in all competitive and recreational activities.\n",
      "- Show respect for opponents, officials, and volunteers.\n",
      "- Cooperate with teammates, coaches, and opponents.\n",
      "\n",
      "**Content and Behavior:**\n",
      "- Use only appropriate and relevant content.\n",
      "- Be mindful of your own words and actions, and avoid spreading misinformation or hate speech.\n",
      "- Report any inappropriate or offensive behavior or content immediately.\n",
      "\n",
      "**Communication:**\n",
      "- Keep your posts relevant to Cork City FC and the local football community.\n",
      "- Be respectful and courteous when communicating with other members.\n",
      "- Avoid self-promotion or spam.\n",
      "\n",
      "**Community Building:**\n",
      "- Engage with the club and its supporters by commenting on news, events, and player updates.\n",
      "- Support the team by attending matches and volunteering your time.\n",
      "- Promote a positive and inclusive atmosphere.\n",
      "\n",
      "**Privacy and Security:**\n",
      "- Respect the privacy of\n",
      "(' ',)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "import re\n",
    "\n",
    "\n",
    "# def extract_text_after_response_start(full_text):\n",
    "#     parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n",
    "#     if len(parts) > 1:\n",
    "#         return parts[1].strip()  # Return text after the last occurrence of response_start\n",
    "#     else:\n",
    "#         return full_text  # Return the original text if response_start is not found\n",
    "\n",
    "eos_token=\"</s>\"\n",
    "base_line=base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.' \n",
    "\n",
    "def extract_text_after_response_start(full_text):\n",
    "    parts = re.search(f\"{response_start}(.*?){eos_token}\", full_text).groups()\n",
    "    print(full_text)\n",
    "    print(parts)\n",
    "    if parts:\n",
    "        return parts\n",
    "    else:\n",
    "        return base_line\n",
    "        \n",
    "\n",
    "def sample_pred(sample):\n",
    "    prompt = get_prompt(sample[\"original_text\"],sample['rewritten_text'])\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        # with torch.inference_mode():\n",
    "        # outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "        outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=1024, do_sample=False)\n",
    "        \n",
    "        rewrite_prompt_pred=extract_text_after_response_start(tokenizer.batch_decode(outputs.detach().cpu().numpy())[0])\n",
    "    \n",
    "    # print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    # print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")\n",
    "    # print(f\"\\nGround truth:\\n{sample['rewrite_prompt']}\")\n",
    "    \n",
    "    # print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    # print(\"**********************\")\n",
    "    # print(\"**********************\")\n",
    "    # print(f\"\\nGenerated instruction:\\n{rewrite_prompt_pred}\")\n",
    "    # print(\"**********************\")\n",
    "    # print(\"**********************\")\n",
    "    # print(f\"\\nGround truth:\\n{sample['rewrite_prompt']}\")\n",
    "\n",
    "for i in range(3):\n",
    "    sample = ds[\"train\"][randrange(len(train_dataset ))]\n",
    "    sample_pred(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6528325-0f19-45d7-b1b2-864274dfa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokenizer.batch_decode(outputs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90303d65-5940-4795-bd7f-26cb83745df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Original Text:  Albuquerque police have released lapel camera footage that shows an officer shooting at a fleeing suspect. In October 2013, officers were called about an erratic driver who was running drivers off the road and firing a weapon from his vehicle. Officers said the man, Joaquin Ortega, tried to carjack drivers after crashing his own car. The newly-released footage seems to contradict the story that police initially reported. Then-Interim Police Chief Allen Banks would not release more than a single frame of the lapel video after the incident, stating that he wasn’t going to allow the case to be tried in [/INST] \n",
      " The request was:  Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.  [INST] \n",
      "Rewritten Text:  \n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  [INST] \n",
      "Rewrite the message as a vintage advertisement:\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing weapon from vehicle.\n",
      "\n",
      "Don't miss this thrilling spectacle! Witness the dramatic police chase and deadly shootout that shook Albuquerque to its core!\n",
      "\n",
      "Release the footage! Demand justice for the innocent victims!\n",
      "\n",
      "#PoliceShooting #Albuquerque #JoaquinOrtega [/INST] \n",
      " The request was:  {'Rewrite the message as a vintage advertisement'} \n",
      " The response:  {'Here is the message rewritten as a vintage advertisement:'}\n",
      "\n",
      "**DANGER! POLICE SHOOTING AT SUSPECT**\n",
      "\n",
      "Wanted: Joaquin Ortega\n",
      "Age: 32\n",
      "Last known: Driving erratically, firing\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b9692-3ebb-45d7-8763-2c5e2c307653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
