{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139b6d98-b350-4ef9-a824-88b6d2e3ff34",
   "metadata": {},
   "source": [
    "# Mistral Guanoca LoRA PEFT example\n",
    "\n",
    "Source Article: https://www.datacamp.com/tutorial/mistral-7b-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89e65d2-d0ef-428a-9582-537ff3784f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.41s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 13:50:26.586772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 13:50:26.586816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 13:50:26.587825: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 13:50:26.592684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 13:50:27.116236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "        28804,   733, 28748, 16289, 28793,  6824, 28725,   315, 28742, 28719,\n",
      "         3448, 10473,   298,   264,  1179, 11322, 19961,   302,  6138, 23598,\n",
      "        18342, 28723,   661, 13633,   776,   272,  1103,  3558,   302,   686,\n",
      "        16944, 15637,   423,   298,  5681,   315, 28742, 28719, 13198,   582,\n",
      "          297,   272,  6132, 28808,     2, 28705,   733, 16289, 28793,  2378,\n",
      "          368,   506,   993,  7136,   864, 21116, 28804,   733, 28748, 16289,\n",
      "        28793])\n",
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>  [INST] Do you have mayonnaise recipes? [/INST] Of course! Here is a simple recipe for mayonnaise that you can try at home:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 large egg yolks\n",
      "* 1 tablespoon dijon mustard\n",
      "* 1 tablespoon white wine vinegar or lemon juice\n",
      "* 1/2 cup vegetable oil\n",
      "* Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Add the egg yolks, mustard, and vinegar to a blender or food processor.\n",
      "2. On a slow speed, gradually blend in the oil until emulsified.\n",
      "3. Season with salt and pepper to taste.\n",
      "4. Chill in the refrigerator before using.\n",
      "\n",
      "And there you have it! A homemade mayonnaise that's perfect for sandwiches, salads, and more.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>  [INST] Do you have mayonnaise recipes? [/INST] Of course! Here is a simple recipe for mayonnaise that you can try at home:\\n\\nIngredients:\\n\\n* 2 large egg yolks\\n* 1 tablespoon dijon mustard\\n* 1 tablespoon white wine vinegar or lemon juice\\n* 1/2 cup vegetable oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Add the egg yolks, mustard, and vinegar to a blender or food processor.\\n2. On a slow speed, gradually blend in the oil until emulsified.\\n3. Season with salt and pepper to taste.\\n4. Chill in the refrigerator before using.\\n\\nAnd there you have it! A homemade mayonnaise that's perfect for sandwiches, salads, and more.</s>\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#model_name=\"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          trust_remote_code=True,\n",
    "                                          device_map=\"auto\"\n",
    "                                         )\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "print(tokenizer.add_bos_token, tokenizer.add_eos_token)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "\n",
    "model.config.use_cache = False # silence the warnings\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "def chat(messages):\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    \n",
    "    model_inputs = encodeds.to(torch.cuda.current_device())\n",
    "    \n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=2000, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    print(encodeds[0])\n",
    "    print(decoded[0])\n",
    "    return decoded[0]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedad57b-1414-48e1-9d32-d4f1315f6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "\n",
    "# df=pd.read_csv('gemma1000_w7b.csv')\n",
    "\n",
    "# df.columns\n",
    "# ds = load_dataset(\"csv\",data_files={\"train\":'gemma1000_w7b.csv'})\n",
    "ds = load_dataset(\"csv\",data_files={\"train\":'gemma10000.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815cd33e-f94f-4076-902e-8ec1dab1b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df[['original_text','gemma_7b_rewritten_text_temp0','rewrite_prompt']]\n",
    "# df['diff_prompt']='Tell me what prompt converted the following text: \\n\"\"\"' + df['original_text'] + '\"\"\"\\n to this text \\n\"\"\"' + df['gemma_7b_rewritten_text_temp0'] + '\"\"\"\\n'\n",
    "#original text prefix\n",
    "import json\n",
    "\n",
    "orig_prefix = \"Original Text: \"\n",
    "\n",
    "#mistral \"response\"\n",
    "llm_response_for_rewrite = \"Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.\"\n",
    "\n",
    "#modified text prefix\n",
    "rewrite_prefix = \"Rewritten Text: \"\n",
    "\n",
    "#provided as start of Mistral response (anything after this is used as the prompt)\n",
    "#providing this as the start of the response helps keep things relevant\n",
    "response_start = \"The request was: \"\n",
    "\n",
    "#We insert our detected prompt into this well-scoring baseline text\n",
    "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
    "base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.' \n",
    "base_line_swap_text = \"[insert desired style here]\"\n",
    "\n",
    "# this function is used to output the right formate for each row in the dataset\n",
    "def create_text_row(instruction, output) -> str:\n",
    "    if output is not None:\n",
    "        text_row = f\"\"\"<s>[INST] {instruction} [/INST] \\\\n {response_start} {output} </s>\"\"\"\n",
    "    else:\n",
    "        text_row = f\"\"\"<s>[INST] {instruction} [/INST] \\\\n {response_start}\"\"\"\n",
    "    return text_row\n",
    "\n",
    "\n",
    "def get_prompt(orig_text, transformed_text, rewritten_prompt = None):\n",
    "    if rewritten_prompt is None:\n",
    "        message = create_text_row(f\"{orig_prefix} {orig_text}\",llm_response_for_rewrite) + create_text_row(f\"{rewrite_prefix} {transformed_text}\", None)\n",
    "    else:\n",
    "        message = create_text_row(f\"{orig_prefix} {orig_text}\",llm_response_for_rewrite) + create_text_row(f\"{rewrite_prefix} {transformed_text}\",{rewritten_prompt})\n",
    "    # if rewritten_prompt is None:\n",
    "    #     message = create_text_row(f\"{orig_prefix} {orig_text}\\n{rewrite_prefix} {transformed_text}\", None)\n",
    "    # else:\n",
    "    #     message = create_text_row(f\"{orig_prefix} {orig_text}\\n{rewrite_prefix} {transformed_text}\",{rewritten_prompt})\n",
    "    return message\n",
    "\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for item in ds[\"train\"]:\n",
    "            json_object = {\n",
    "                \"text\": get_prompt(item[\"original_text\"],item['rewritten_text'],item['rewrite_prompt'])\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./training_dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac41d4be-7887-46c8-a18d-480790dbe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 9999 examples [00:00, 167666.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files='./training_dataset.jsonl' , split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093ab2f1-d71e-43ed-ae56-0ca366a75287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(x):\n",
    "#     model_inputs = tokenizer.apply_chat_template(x, return_tensors=\"pt\")    \n",
    "#     generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "#     decoded = tokenizer.batch_decode(generated_ids)\n",
    "#     return decoded\n",
    "\n",
    "# for i,r in df.iterrows():\n",
    "#     message=[\n",
    "#     {\"role\": \"user\", \"content\": r['original_text']},\n",
    "#     {\"role\": \"assistant\", \"content\": r[\n",
    "#     ]\n",
    "#     df.at[i, 'predicted_prompt']=chat(message)\n",
    "#     print(df.at[i, 'predicted_prompt'])\n",
    "#     if (i>5):\n",
    "#         break\n",
    "\n",
    "# df.iloc[0:6,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f109dc69-0bcc-499c-b3e5-63570fc673d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da41dc8-f7b0-4703-8613-a8c7018726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=128,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af4f26a-1c12-47ae-b6cd-20a9e5fe6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_total_limit = 2, \n",
    "    save_strategy = \"no\", \n",
    "    load_best_model_at_end=False,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c743de9-b60c-4b26-a7f4-062561b64e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9999/9999 [00:01<00:00, 9295.38 examples/s]\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a27ea0-ddb2-451e-ab5b-23da781a745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3913' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3913/5000 2:53:09 < 48:07, 0.38 it/s, Epoch 1.56/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.284200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model='Mistral-7b-instruct-v0.1-qlora'\n",
    "#new_model='Mistral-7b-v0.1-qlora'\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf982bc7-4e7a-471e-a8d7-9716b01822cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.43s/it]\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.model',\n",
       " 'merged_model/added_tokens.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the model with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16, #bfloat and float?\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff820a3-8538-476a-a637-0b73eed11029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c814bf9-ab0b-457a-a6d5-386a553494a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How do I find true love? [/INST] Finding true love is a journey that requires time, effort, and self-discovery. Here are some steps you can take to increase your chances of finding true love:\n",
      "\n",
      "1. Focus on self-improvement: Work on becoming the best version of yourself. This includes developing your interests, hobbies, and passions, as well as working on your personality traits and habits.\n",
      "2. Set clear boundaries: Be clear about what you want in a partner and what you are willing to compromise on. This will help you attract the right person and avoid wasting time on unhealthy relationships.\n",
      "3. Expand your social circle: Meet new people through social events, online dating, or by joining clubs or groups that share your interests.\n",
      "4. Be open-minded: Keep an open mind and be willing to try new things. This includes being open to different types of people and\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"How do I find true love?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3a42fbc-b75b-4c7b-b766-ac272afae12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] Original Text:  On Oct. 15, 2010, Fenway Sports Group completed their purchase of Liverpool Football Club. Since, the Anfield outfit has sacked a pair of managers, spent a significant sum of money and nearly secured their first English top-flight trophy since 1990. Here are some key stats that illustrate FSG's ownership of Liverpool on their five-year anniversary. The right signings? Since FSG's takeover, Liverpool have been an active club in the transfer market. However, while the club has been aggressive in recruiting young talent with resale value, their success rate in recruitment has been suspect to say the least. Sibling spending sprees [/INST] \\n The request was:  Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral. </s><s>[INST] Rewritten Text:  **Project Title:** Fenway's Reign at Anfield\n",
      "\n",
      "**Objective:** To assess the impact of Fenway Sports Group's ownership on Liverpool Football Club over the past five years.\n",
      "\n",
      "**Key Tasks:**\n",
      "1. Analyze financial statements and sporting performance data to evaluate FSG's ownership decisions and their overall impact on the club.\n",
      "2. Review the transfer market activity of Liverpool during this period, focusing on both successful and unsuccessful signings.\n",
      "3. Assess the impact of FSG's ownership on the club's image and fan base.\n",
      "4. Evaluate the long-term sustainability of Liverpool's success under FSG's stewardship.\n",
      "5. Provide recommendations for future ownership strategies that would maximize the value and success of the club. [/INST] \\n The request was: \n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "[INST] Original Text:  On Oct. 15, 2010, Fenway Sports Group completed their purchase of Liverpool Football Club. Since, the Anfield outfit has sacked a pair of managers, spent a significant sum of money and nearly secured their first English top-flight trophy since 1990. Here are some key stats that illustrate FSG's ownership of Liverpool on their five-year anniversary. The right signings? Since FSG's takeover, Liverpool have been an active club in the transfer market. However, while the club has been aggressive in recruiting young talent with resale value, their success rate in recruitment has been suspect to say the least. Sibling spending sprees [/INST] \\n The request was:  Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.  [INST] Rewritten Text:  **Project Title:** Fenway's Reign at Anfield\n",
      "\n",
      "**Objective:** To assess the impact of Fenway Sports Group's ownership on Liverpool Football Club over the past five years.\n",
      "\n",
      "**Key Tasks:**\n",
      "1. Analyze financial statements and sporting performance data to evaluate FSG's ownership decisions and their overall impact on the club.\n",
      "2. Review the transfer market activity of Liverpool during this period, focusing on both successful and unsuccessful signings.\n",
      "3. Assess the impact of FSG's ownership on the club's image and fan base.\n",
      "4. Evaluate the long-term sustainability of Liverpool's success under FSG's stewardship.\n",
      "5. Provide recommendations for future ownership strategies that would maximize the value and success of the club. [/INST] \\n The request was: \n",
      "\n",
      "Ground truth:\n",
      "Rewrite this as an engineer's project.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "sample = ds[\"train\"][randrange(len(train_dataset ))]\n",
    "\n",
    "prompt = get_prompt(sample[\"original_text\"],sample['rewritten_text'])\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "# outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=16, do_sample=False)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['rewrite_prompt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6528325-0f19-45d7-b1b2-864274dfa51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
