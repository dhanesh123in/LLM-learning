{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139b6d98-b350-4ef9-a824-88b6d2e3ff34",
   "metadata": {},
   "source": [
    "# Seq2Seq LLM Reprompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedad57b-1414-48e1-9d32-d4f1315f6f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "\n",
    "# df.columns\n",
    "ds = load_dataset(\"csv\",data_files={\"train\":'gemma1000_w7b.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815cd33e-f94f-4076-902e-8ec1dab1b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df[['original_text','gemma_7b_rewritten_text_temp0','rewrite_prompt']]\n",
    "# df['diff_prompt']='Tell me what prompt converted the following text: \\n\"\"\"' + df['original_text'] + '\"\"\"\\n to this text \\n\"\"\"' + df['gemma_7b_rewritten_text_temp0'] + '\"\"\"\\n'\n",
    "#original text prefix\n",
    "\n",
    "orig_prefix = \"Original Text:\"\n",
    "\n",
    "rewrite_prefix = \"\\nRewritten Text:\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_prompt(orig_text, transformed_text):\n",
    "    #message = f\"{orig_prefix} {orig_text} {rewrite_prefix} {transformed_text}\"\n",
    "    message = f\"{rewrite_prefix} {transformed_text}\"\n",
    "    return message\n",
    "\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for item in ds[\"train\"]:\n",
    "            json_object = {\n",
    "                \"text\": get_prompt(item[\"original_text\"],item['gemma_7b_rewritten_text_temp0']),\n",
    "                \"output\": item['rewrite_prompt']\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./training_dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac41d4be-7887-46c8-a18d-480790dbe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000 examples [00:00, 139828.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = load_dataset('json', data_files='./training_dataset.jsonl', split=\"train\")\n",
    "\n",
    "train_dataset=train_dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093ab2f1-d71e-43ed-ae56-0ca366a75287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"google/flan-t5-base\"\n",
    "model_checkpoint = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f58981b-536a-4bcd-9126-3f979497710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 01:00:12.206040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 01:00:12.206083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 01:00:12.207127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-22 01:00:12.212079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 01:00:12.832315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0282eb9-6483-40ed-b942-57b0d53310bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fd4f52-70a7-4b9f-84da-e8d42b72ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 16\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"output\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62e96b0-6a22-48f0-b364-c10a170f77ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:00<00:00, 8196.09 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 7017.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3d54b6-145f-4be1-bfcd-ac1f8728b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f637e9d-87c7-4aaa-970f-5a6e25998456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16048b24-1272-4db4-9dd4-f20e1bf3e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-reprompt\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #fp16_full_eval=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9aa8144-98b0-4eb5-94f5-ec263c2f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c177083-8e15-4afc-832b-be1586abc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1160d4f5-a089-4755-9f96-9a0dad3bd1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],    \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a1ffb6-a234-4a20-b334-bb78f3625d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 13:09, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.216100</td>\n",
       "      <td>0.833756</td>\n",
       "      <td>73.302100</td>\n",
       "      <td>62.757600</td>\n",
       "      <td>73.035700</td>\n",
       "      <td>72.989800</td>\n",
       "      <td>13.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.719490</td>\n",
       "      <td>77.999400</td>\n",
       "      <td>69.562100</td>\n",
       "      <td>77.758700</td>\n",
       "      <td>77.712500</td>\n",
       "      <td>13.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.693895</td>\n",
       "      <td>78.085700</td>\n",
       "      <td>69.120000</td>\n",
       "      <td>77.909800</td>\n",
       "      <td>77.975100</td>\n",
       "      <td>13.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.703615</td>\n",
       "      <td>78.152900</td>\n",
       "      <td>70.225200</td>\n",
       "      <td>78.106800</td>\n",
       "      <td>78.165800</td>\n",
       "      <td>13.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.416100</td>\n",
       "      <td>0.694739</td>\n",
       "      <td>79.638000</td>\n",
       "      <td>71.964200</td>\n",
       "      <td>79.522300</td>\n",
       "      <td>79.511100</td>\n",
       "      <td>12.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.696823</td>\n",
       "      <td>78.520400</td>\n",
       "      <td>70.241600</td>\n",
       "      <td>78.541000</td>\n",
       "      <td>78.513500</td>\n",
       "      <td>13.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>80.445500</td>\n",
       "      <td>72.879200</td>\n",
       "      <td>80.501700</td>\n",
       "      <td>80.478500</td>\n",
       "      <td>13.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.701242</td>\n",
       "      <td>79.825400</td>\n",
       "      <td>72.031800</td>\n",
       "      <td>79.786100</td>\n",
       "      <td>79.743100</td>\n",
       "      <td>13.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.699299</td>\n",
       "      <td>80.006400</td>\n",
       "      <td>72.141800</td>\n",
       "      <td>79.944800</td>\n",
       "      <td>79.899000</td>\n",
       "      <td>13.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory bart-large-finetuned-reprompt/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory bart-large-finetuned-reprompt/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory bart-large-finetuned-reprompt/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4500, training_loss=0.5263974372016059, metrics={'train_runtime': 790.225, 'train_samples_per_second': 22.778, 'train_steps_per_second': 5.695, 'total_flos': 1.4731418378502144e+16, 'train_loss': 0.5263974372016059, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c194e093-71f5-4189-ab14-66af2b4ae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('Seq2Seq_bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41bb6b2-5505-49ef-aaeb-da37301a0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained('./Seq2Seq_tst', device_map=\"auto\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./Seq2Seq_bart-large', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93dc51fb-08b9-40e3-965b-7794fed80fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    input_ids = tokenizer(x, return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_target_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b230bec-b1e3-4a63-b736-d99af7a95580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this into a passage from ancient scriptures.\n",
      "Translate this into a text from ancient scriptures.\n",
      "\n",
      "Rewritten Text: Sure, here is the text translated into ancient scriptures:\n",
      "\n",
      "\"In the days of old, when the internet was a new and wondrous thing, there lived a group of major rights holders who claimed that the search engines were making it \"difficult\" for people to find legal music and films online.\n",
      "\n",
      "One day, a coalition of entertainment industry groups approached the government and presented a confidential document that revealed the truth. The document claimed that Google and Microsoft's Bing were \"overwhelmingly\" directing music fans to illegal copies of copyrighted tracks.\n",
      "\n",
      "The major rights holders argued that the search engines were making it \"much more difficult\" for people to find legal music and films. They claimed that this was a violation of their rights and that the government had a responsibility to protect them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/envs/test_env/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(pred(train_dataset[\"train\"][0][\"text\"]))\n",
    "print(train_dataset[\"train\"][0][\"output\"])\n",
    "print(train_dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ff820a3-8538-476a-a637-0b73eed11029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrase this as a sitcom scene.\n",
      "Rephrase this as a sitcom scene.\n",
      "\n",
      "Rewritten Text: Sure, here is the rephrased text as a sitcom scene:\n",
      "\n",
      "[FADE IN]\n",
      "\n",
      "**INT. COFFEE SHOP - DAY**\n",
      "\n",
      "A young entrepreneur, MARCIA (20s), sits in a coffee shop, staring at a laptop. She's frowning and frustrated. A waiter, BRENDA (20s), approaches her table.\n",
      "\n",
      "**BRENDA:** Can I get you anything else, Miss?\n",
      "\n",
      "**MARCIA:** (snapping) You're not going to believe this, Brenda. I've been trying to license an invention from the NIH, and it's a real mess.\n",
      "\n",
      "**BRENDA:** Oh no, I'm sorry to hear that, Marcia. What's up?\n",
      "\n",
      "**MARCIA:** The invention is a system that can predict your psychological status based on your smartphone usage. It's a big deal, but the government is making it hard to get it off the ground.\n",
      "\n",
      "**BRENDA:** Really? That sounds like a very cool idea.\n",
      "\n",
      "**MARCIA:** I know, right? But the government is only interested in licensing it to big companies, not small ones. It's a real slap in the face.\n",
      "\n",
      "**BRENDA:** (sympathetic) I understand why you're frustrated, Marcia.\n",
      "\n",
      "**MARCIA:** (taking a sip of coffee) I guess I'll just have to keep trying. I've got a good feeling this one will make it big.\n",
      "\n",
      "**BRENDA:** Well, I'm sure you'll be successful.\n",
      "\n",
      "Marcia smiles and nods at Brenda.\n",
      "\n",
      "**MARCIA:** Thanks, Brenda. I appreciate you listening.\n",
      "\n",
      "**BRENDA:** No problem, Marcia. I'm always happy to lend an ear.\n",
      "\n",
      "Marcia takes a bite of her coffee and continues to work on her laptop.\n",
      "\n",
      "[FADE OUT]\n"
     ]
    }
   ],
   "source": [
    "print(pred(train_dataset[\"train\"][12][\"text\"]))\n",
    "print(train_dataset[\"train\"][12][\"output\"])\n",
    "print(train_dataset[\"train\"][12][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6528325-0f19-45d7-b1b2-864274dfa51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rewrite this as a shanty to be sung.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(\"\"\"\\nRewritten Text: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2381e5b-18fe-485d-8da7-3956e4c071b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
